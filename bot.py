# bot.py
import os
import time
import logging
import random
import re
from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse

import cloudscraper
from bs4 import BeautifulSoup

# =============== –ù–ê–°–¢–†–û–ô–ö–ò ===============
EBAY_URLS = [
    # —Ç–≤–æ–π –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ–∏—Å–∫
    "https://www.ebay.com/sch/i.html?_udlo=100&_nkw=garmin+astro+320+&_sacat=0&_stpos=19720&_fcid=1",
]

CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL", "180"))   # 3 –º–∏–Ω
PAGES_TO_SCAN = 3                                         # pgn=1..3
REQUEST_TIMEOUT = 20
RETRIES_PER_PAGE = 2
RETRY_SLEEP = (2, 4)     # —Å–ª—É—á–∞–π–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ä–µ—Ç—Ä–∞—è–º–∏
UA_ROTATE = [
    # –ø–∞—Ä–∞ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö UA; –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–≤–æ–∏
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Safari/605.1.15",
]

# Telegram –º–æ–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –ø–æ–∑–∂–µ; —Å–µ–π—á–∞—Å –Ω–µ —à–ª—ë–º —Å–æ–æ–±—â–µ–Ω–∏—è
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = os.getenv("CHAT_ID", "")

# =============== –õ–û–ì–ò ===============
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
)
log = logging.getLogger("ebay")

# =============== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–û–ï ===============
def normalize_search_url(url: str) -> str:
    """
    –ß–∏—Å—Ç–∏–º URL –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Ñ–∏–∫—Å–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ (_ipg=240, rt=nc, _pgn=N).
    –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ ¬´–±–µ–ª—ã–π —Å–ø–∏—Å–æ–∫¬ª —Ñ–∏–ª—å—Ç—Ä–æ–≤ eBay.
    """
    allowed = {
        "_nkw", "_sacat", "_dcat", "_udlo", "_udhi", "_stpos", "_fcid", "_sop", "_nqc",
    }
    u = urlparse(url)
    params = dict(parse_qsl(u.query, keep_blank_values=True))

    # –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    cleaned = {k: v for k, v in params.items() if k in allowed}

    # –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –≤—ã–¥–∞—á–∏
    cleaned["_ipg"] = "240"
    cleaned["rt"] = "nc"

    # —Å–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ (–±–µ–∑ _pgn)
    q = urlencode(cleaned, doseq=True)
    base = urlunparse((u.scheme, u.netloc, u.path, "", q, ""))
    return base

def make_scraper():
    # cloudscraper —É–º–µ–µ—Ç –æ–±—Ö–æ–¥–∏—Ç—å cloudflare/js-—á–µ–ª–ª–µ–Ω–¥–∂–∏
    scraper = cloudscraper.create_scraper(
        browser={
            "browser": "chrome",
            "platform": "windows",
            "desktop": True
        }
    )
    # –±–∞–∑–æ–≤—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
    headers = {
        "User-Agent": random.choice(UA_ROTATE),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Connection": "keep-alive",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Upgrade-Insecure-Requests": "1",
    }
    scraper.headers.update(headers)
    return scraper

def fetch_page_html(scraper, url) -> str:
    """
    –°–∫–∞—á–∏–≤–∞–µ–º HTML —Å —Ä–µ—Ç—Ä–∞—è–º–∏. –ï—Å–ª–∏ 503/403 ‚Äî –º–µ–Ω—è–µ–º UA –∏ –∂–¥—ë–º.
    """
    last_err = None
    for attempt in range(1, RETRIES_PER_PAGE + 1):
        try:
            scraper.headers["User-Agent"] = random.choice(UA_ROTATE)
            r = scraper.get(url, timeout=REQUEST_TIMEOUT)
            if r.status_code >= 500:
                raise RuntimeError(f"{r.status_code} Server Error")
            if r.status_code in (403, 429):
                raise RuntimeError(f"{r.status_code} Rate/Captcha")
            return r.text
        except Exception as e:
            last_err = e
            log.warning(f"–û—à–∏–±–∫–∞ HTML –Ω–∞ {url} (attempt {attempt}/{RETRIES_PER_PAGE}): {e}")
            time.sleep(random.uniform(*RETRY_SLEEP))
    # —É–ø–∞–ª–æ –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ
    raise last_err or RuntimeError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")

def parse_items_from_html(html: str):
    """
    –ü–∞—Ä—Å–∏–º –∫–∞—Ä—Ç–æ—á–∫–∏ –∏–∑ HTML.
    1) –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä: li.s-item
    2) –∑–∞–ø–∞—Å–Ω–æ–π: div.s-item__wrapper
    –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å: id (itemId –∏–∑ —Å—Å—ã–ª–∫–∏), title, price, link
    """
    soup = BeautifulSoup(html, "html.parser")

    # –ø—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É/—á–µ–ª–ª–µ–Ω–¥–∂
    text_low = soup.get_text(" ", strip=True).lower()
    if "verify you're a human" in text_low or "captcha" in text_low:
        return [], True  # –∫–∞–ø—á–∞

    items = soup.select("li.s-item")
    if len(items) < 5:
        # fallback
        alt = soup.select("div.s-item__wrapper")
        if len(alt) > len(items):
            items = alt

    parsed = []
    for it in items:
        a = it.select_one("a.s-item__link")
        title_tag = it.select_one("h3.s-item__title")
        price_tag = it.select_one(".s-item__price")
        if not a or not title_tag:
            continue
        link = a.get("href", "").strip()
        title = title_tag.get_text(strip=True)

        # —Ü–µ–Ω–∞ –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–∞ —á–∞—Å—Ç–∏ –∫–∞—Ä—Ç–æ—á–µ–∫ (—Ä–µ–∫–ª–∞–º–∞/–≤–∏—Ç—Ä–∏–Ω–∞)
        price = price_tag.get_text(strip=True) if price_tag else ""

        # –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º itemId –∏–∑ —Å—Å—ã–ª–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        m = re.search(r"/(\d{9,})\?", link)
        item_id = m.group(1) if m else link  # fallback ‚Äî –≤—Å—è —Å—Å—ã–ª–∫–∞

        parsed.append({
            "id": item_id,
            "title": title,
            "price": price,
            "link": link,
        })
    return parsed, False

# –ø–∞–º—è—Ç—å –æ —É–∂–µ —É–≤–∏–¥–µ–Ω–Ω—ã—Ö ID (–Ω–∞ –∫–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É)
seen = {}

def crawl_search(url: str):
    """
    –ì—Ä—É–∑–∏–º –¥–æ PAGES_TO_SCAN —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–∏—Å–∫–æ–≤–æ–π –≤—ã–¥–∞—á–∏. –ï—Å–ª–∏ HTML –¥–∞—ë—Ç –º–∞–ª–æ –∫–∞—Ä—Ç–æ—á–µ–∫,
    —ç—Ç–æ –ø–æ—á—Ç–∏ –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ –∑–∞–≥–ª—É—à–∫–∞/–∫–∞–ø—á–∞ ‚Äî –ª–æ–≥–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
    """
    base = normalize_search_url(url)
    log.info(f"Base URL: {base}")
    scraper = make_scraper()

    all_items = []
    human_check_detected = False

    for p in range(1, PAGES_TO_SCAN + 1):
        page_url = f"{base}&_pgn={p}"
        log.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {p}/{PAGES_TO_SCAN}: {page_url}")
        try:
            html = fetch_page_html(scraper, page_url)
        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ HTML p={p}: {e}")
            continue

        items, found_captcha = parse_items_from_html(html)
        log.info(f"HTML p={p}: –Ω–∞–π–¥–µ–Ω–æ {len(items)} –∫–∞—Ä—Ç–æ—á–µ–∫")
        all_items.extend(items)
        human_check_detected = human_check_detected or found_captcha

        # –µ—Å–ª–∏ –∫–∞—Ä—Ç–æ—á–µ–∫ —Å–æ–≤—Å–µ–º –º–∞–ª–æ ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if len(items) < 5:
            try:
                dump_path = f"/opt/render/project/src/ebay_debug_p{p}.html"
                with open(dump_path, "w", encoding="utf-8") as f:
                    f.write(html)
                log.warning(f"–°–æ—Ö—Ä–∞–Ω—ë–Ω HTML –¥–∞–º–ø –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {dump_path}")
            except Exception as e:
                log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–º–ø HTML: {e}")

        # –Ω–µ–±–æ–ª—å—à–∞—è —Ä–∞–Ω–¥–æ–º–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
        time.sleep(random.uniform(1.0, 2.5))

    # –£–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ id
    uniq = {}
    for it in all_items:
        uniq[it["id"]] = it
    all_items = list(uniq.values())

    if human_check_detected:
        log.warning("–ü–æ—Ö–æ–∂–µ –Ω–∞ –∑–∞—â–∏—Ç–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É (captcha/human check). –ö–æ–ª-–≤–æ –∫–∞—Ä—Ç–æ—á–µ–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–Ω–∏–∂–µ–Ω–æ.")

    log.info(f"–ò—Ç–æ–≥: —Å–æ–±—Ä–∞–Ω–æ {len(all_items)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–ø–æ—Å–ª–µ —á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)")
    return all_items

def main():
    global seen
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â—É—é –≤—ã–¥–∞—á—É –∏ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ —É–∂–µ –≤–∏–¥–µ–Ω–Ω—É—é
    for url in EBAY_URLS:
        try:
            items = crawl_search(url)
            seen[url] = {it["id"] for it in items}
            log.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(seen[url])} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ {url}")
        except Exception as e:
            log.warning(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {url}: {e}")
            seen[url] = set()

    check_num = 0
    while True:
        check_num += 1
        log.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ #{check_num} –Ω–∞—á–∞—Ç–∞")
        for url in EBAY_URLS:
            try:
                items = crawl_search(url)
                new_items = [it for it in items if it["id"] not in seen[url]]
                log.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(items)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –Ω–æ–≤—ã—Ö={len(new_items)} –ø–æ {url}")

                # –û–±–Ω–æ–≤–ª—è–µ–º ¬´—É–≤–∏–¥–µ–Ω–Ω—ã–µ¬ª
                for it in new_items:
                    seen[url].add(it["id"])

                # –∑–¥–µ—Å—å –ø–æ–∑–∂–µ –º–æ–∂–Ω–æ –≤–∫–ª—é—á–∏—Ç—å –æ—Ç–ø—Ä–∞–≤–∫—É –≤ Telegram
                # for it in new_items:
                #     send_telegram_message(f"üÜï {it['title']}\n{it['price']}\n{it['link']}")

            except Exception as e:
                log.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è {url}: {e}")

            # –Ω–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
            time.sleep(random.uniform(1.0, 2.5))

        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏
        time.sleep(CHECK_INTERVAL)

if __name__ == "__main__":
    log.info(f"–°–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω. –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–æ–∫: {CHECK_INTERVAL}s")
    main()
