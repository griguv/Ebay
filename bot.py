import time
import os
import logging
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse
import xml.etree.ElementTree as ET

import cloudscraper
from bs4 import BeautifulSoup
import requests

# ---------------- –ö–æ–Ω—Ñ–∏–≥ ----------------
EBAY_URLS = [
    "https://www.ebay.com/sch/i.html?_udlo=100&_nkw=garmin+astro+320+&_sacat=0",
]

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_IDS = os.getenv("CHAT_ID", "").split(",")

CHECK_INTERVAL = 180       # –∫–∞–∂–¥—ã–µ 3 –º–∏–Ω—É—Ç—ã
REPORT_INTERVAL = 1800     # –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç
MAX_PAGES = 3              # —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ —Å–º–æ—Ç—Ä–µ—Ç—å
MIN_HTML_ITEMS = 5         # –µ—Å–ª–∏ –º–µ–Ω—å—à–µ ‚Äî –ø—Ä–æ–±—É–µ–º RSS

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36"
    )
}

# ---------------- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ----------------
logging.basicConfig(
    format="%(asctime)s | %(levelname)s | %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger()

# ---------------- –ü—Ä–æ–∫—Å–∏ ----------------
PROXIES = []  # —Å—é–¥–∞ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏
def _proxy_iter_cycle():
    while True:
        for p in [None] + PROXIES:
            if not p:
                yield None
            else:
                yield {"http": p, "https": p}
_proxy_iter = _proxy_iter_cycle()

# ---------------- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------------
def make_scraper_with_retries():
    return cloudscraper.create_scraper(browser="chrome")

def _with_params(url: str, extra: dict) -> str:
    u = urlparse(url)
    q = dict(parse_qsl(u.query, keep_blank_values=True))
    q.update(extra)
    new_q = urlencode(q, doseq=True)
    return urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))

def _strip_params(url: str, keys_to_drop: list[str]) -> str:
    u = urlparse(url)
    q = dict(parse_qsl(u.query, keep_blank_values=True))
    for k in keys_to_drop:
        q.pop(k, None)
    new_q = urlencode(q, doseq=True)
    return urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))

def _fetch_html_page(url: str):
    scraper = make_scraper_with_retries()
    current_proxy = next(_proxy_iter)
    resp = scraper.get(url, headers=HEADERS, proxies=current_proxy, timeout=(20, 45))
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    raw_cards = soup.select("li.s-item")
    valid_items = []
    for card in raw_cards:
        title_tag = card.select_one(".s-item__title")
        link_tag = card.select_one(".s-item__link")
        if not title_tag or not link_tag:
            continue
        price_tag = card.select_one(".s-item__price")
        title = title_tag.get_text(strip=True)
        link = link_tag.get("href", "")
        price = price_tag.get_text(strip=True) if price_tag else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
        item_id = link.split("/")[-1].split("?")[0] or link
        valid_items.append({"id": item_id, "title": title, "price": price, "link": link})
    logger.info(f"HTML: –≤—Å–µ–≥–æ –∫–∞—Ä—Ç–æ—á–µ–∫={len(raw_cards)}, –≤–∞–ª–∏–¥–Ω—ã—Ö={len(valid_items)}")
    return valid_items

def _fetch_via_rss(url: str):
    rss_url = _with_params(url, {"_rss": "1"})
    scraper = make_scraper_with_retries()
    current_proxy = next(_proxy_iter)
    resp = scraper.get(rss_url, headers=HEADERS, proxies=current_proxy, timeout=(20, 45))
    resp.raise_for_status()
    root = ET.fromstring(resp.text)
    items = []
    for it in root.findall(".//item"):
        title = (it.findtext("title") or "").strip()
        link = (it.findtext("link") or "").strip()
        if not title or not link:
            continue
        price = "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
        item_id = link.split("/")[-1].split("?")[0] or link
        items.append({"id": item_id, "title": title, "price": price, "link": link})
    logger.info(f"RSS: –≤–∞–ª–∏–¥–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤={len(items)} (url: {rss_url})")
    return items

def fetch_listings(url: str):
    clean_url = _strip_params(url, ["_stpos", "_fcid"])
    base = _with_params(clean_url, {"_ipg": "240", "rt": "nc"})
    logger.info(f"Base URL after cleanup: {base}")

    aggregated = []
    for p in range(1, MAX_PAGES + 1):
        page_url = _with_params(base, {"_pgn": str(p)})
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {p}/{MAX_PAGES}: {page_url}")
            items = _fetch_html_page(page_url)
            aggregated.extend(items)
            if len(items) == 0:
                break
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ HTML –Ω–∞ p={p}: {e}")
            time.sleep(3)

    if len(aggregated) < MIN_HTML_ITEMS:
        logger.warning(f"HTML –¥–∞–ª –º–∞–ª–æ ({len(aggregated)}) ‚Äî –ø—Ä–æ–±—É—é RSS")
        try:
            rss_items = _fetch_via_rss(url)
            known = {it["id"] for it in aggregated}
            for it in rss_items:
                if it["id"] not in known:
                    aggregated.append(it)
            logger.info(f"–ü–æ—Å–ª–µ RSS –≤—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(aggregated)}")
        except Exception as e:
            logger.warning(f"RSS —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å: {e}")

    dedup = {}
    for it in aggregated:
        dedup[it["id"]] = it
    result = list(dedup.values())
    logger.info(f"–ò—Ç–æ–≥: —Å–æ–±—Ä–∞–Ω–æ {len(result)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (HTML+RSS)")
    return result

# ---------------- Telegram ----------------
def send_telegram_message(message):
    if not BOT_TOKEN:
        return
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    for raw_id in CHAT_IDS:
        chat_id = raw_id.strip()
        if not chat_id:
            continue
        try:
            r = requests.post(url, data={"chat_id": chat_id, "text": message}, timeout=15)
            if r.status_code != 200:
                logger.warning(f"–û—à–∏–±–∫–∞ Telegram ({chat_id}): {r.text}")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ Telegram –¥–ª—è {chat_id}: {e}")

# ---------------- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ----------------
seen_items = {url: set() for url in EBAY_URLS}
last_report_time = datetime.now()
checks_count = 0
new_items_count = 0

logger.info(f"–°–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω. –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–æ–∫: {CHECK_INTERVAL}s. –ü—Ä–æ–∫—Å–∏: {PROXIES or '–Ω–µ—Ç'}")

for url in EBAY_URLS:
    try:
        listings = fetch_listings(url)
        for item in listings:
            seen_items[url].add(item["id"])
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ {url}")
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ {url}: {e}")

while True:
    checks_count += 1
    logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ #{checks_count} –Ω–∞—á–∞—Ç–∞")

    for url in EBAY_URLS:
        try:
            listings = fetch_listings(url)
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ {url}")
            new_for_url = 0
            for item in listings:
                if item["id"] not in seen_items[url]:
                    seen_items[url].add(item["id"])
                    new_items_count += 1
                    new_for_url += 1
                    msg = (
                        "üÜï –ù–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –Ω–∞ eBay!\n"
                        f"üìå {item['title']}\n"
                        f"üí≤ {item['price']}\n"
                        f"üîó {item['link']}"
                    )
                    send_telegram_message(msg)
            logger.info(f"–ù–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ —ç—Ç–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ: {new_for_url}")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {url}: {e}")

    if datetime.now() - last_report_time > timedelta(seconds=REPORT_INTERVAL):
        report = (
            "üìä –û—Ç—á—ë—Ç –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –º–∏–Ω—É—Ç\n"
            f"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"üîé –ü—Ä–æ–≤–µ—Ä–æ–∫: {checks_count}\n"
            f"üÜï –ù–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {new_items_count}\n"
            "‚úÖ –ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç"
        )
        send_telegram_message(report)
        last_report_time = datetime.now()
        checks_count = 0
        new_items_count = 0

    time.sleep(CHECK_INTERVAL)
