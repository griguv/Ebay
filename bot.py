# bot.py
import os
import time
import logging
import random
import re
from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse

import cloudscraper
from bs4 import BeautifulSoup

# ================== –ù–ê–°–¢–†–û–ô–ö–ò ==================
EBAY_URLS = [
    "https://www.ebay.com/sch/i.html?_udlo=100&_nkw=garmin+astro+320+&_sacat=0&_stpos=19720&_fcid=1",
]

CHECK_INTERVAL = int(os.getenv("CHECK_INTERVAL", "180"))  # –∫–∞–∂–¥—ã–µ 3 –º–∏–Ω
PAGES_TO_SCAN = 3
REQUEST_TIMEOUT = 20
RETRIES_PER_PAGE = 3
RETRY_SLEEP = (2.0, 4.0)

UA_ROTATE = [
    # –ü–∞—Ä–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö UA; –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å–ø–∏—Å–æ–∫
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Safari/605.1.15",
]

# Telegram –ø–æ–∫–∞ –æ—Ç–∫–ª—é—á–∞–µ–º (—à—É–º–Ω–æ –ø—Ä–∏ –æ—Ç–ª–∞–¥–∫–µ), –≤–∫–ª—é—á–∏–º –ø–æ–∑–∂–µ
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = os.getenv("CHAT_ID", "")

# ================== –õ–û–ì–ò ==================
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
log = logging.getLogger("ebay")

# ================== –£–¢–ò–õ–ò–¢–´ ==================
def normalize_search_url(url: str) -> str:
    """
    –ß–∏—Å—Ç–∏–º –≤—Ö–æ–¥–Ω–æ–π URL –æ—Ç —Ç—Ä–µ–∫–∏–Ω–≥–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ ¬´–±–µ–ª—ã–π —Å–ø–∏—Å–æ–∫¬ª,
    —Ñ–∏–∫—Å–∏—Ä—É–µ–º _ipg=240 –∏ rt=nc. _pgn –±—É–¥–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å –ø–æ–∑–∂–µ.
    """
    allowed = {
        "_nkw", "_sacat", "_dcat", "_udlo", "_udhi", "_stpos", "_fcid", "_sop", "_nqc",
    }
    u = urlparse(url)
    params = dict(parse_qsl(u.query, keep_blank_values=True))

    cleaned = {k: v for k, v in params.items() if k in allowed}
    cleaned["_ipg"] = "240"
    cleaned["rt"] = "nc"

    q = urlencode(cleaned, doseq=True)
    return urlunparse((u.scheme, u.netloc, u.path, "", q, ""))

def make_scraper():
    s = cloudscraper.create_scraper(
        browser={"browser": "chrome", "platform": "windows", "desktop": True}
    )
    # –ë–∞–∑–æ–≤—ã–µ ¬´–±—Ä–∞—É–∑–µ—Ä–Ω—ã–µ¬ª –∑–∞–≥–æ–ª–æ–≤–∫–∏
    s.headers.update({
        "User-Agent": random.choice(UA_ROTATE),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Connection": "keep-alive",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "Upgrade-Insecure-Requests": "1",
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ client hints ‚Äî —á–∞—Å—Ç–æ –ø–æ–º–æ–≥–∞—é—Ç –º–∏–Ω–æ–≤–∞—Ç—å –∑–∞–≥–ª—É—à–∫–∏
        "Sec-CH-UA": "\"Chromium\";v=\"126\", \"Not=A?Brand\";v=\"24\"",
        "Sec-CH-UA-Mobile": "?0",
        "Sec-CH-UA-Platform": "\"Windows\"",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-User": "?1",
    })
    return s

def looks_like_human_check(soup: BeautifulSoup, full_text_low: str) -> bool:
    # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –º–∞—Ä–∫–µ—Ä—ã
    if ("verify you're a human" in full_text_low or
        "to continue to ebay" in full_text_low or
        "access denied" in full_text_low or
        "captcha" in full_text_low):
        return True
    # –¢–∏–ø–∏—á–Ω—ã–µ DOM-—à–∞–±–ª–æ–Ω—ã –ø—Ä–æ–≤–µ—Ä–æ–∫
    if soup.select_one("#challenge-form, form[action*='challenge']"):
        return True
    if soup.select_one("iframe[src*='captcha'], img[alt*='captcha']"):
        return True
    return False

def fetch_page_html(scraper, page_url: str, referer: str) -> str:
    """
    –¢—è–Ω–µ–º HTML —Å —Ä–µ—Ç—Ä–∞—è–º–∏. –ü—Ä–∏ 503/403 –º–µ–Ω—è–µ–º UA, –∂–¥—ë–º –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞.
    –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–Ω–¥–æ–º–Ω—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫—ç—à–∞ / –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–ª–µ–¥–æ–≤.
    """
    last_err = None
    for attempt in range(1, RETRIES_PER_PAGE + 1):
        try:
            scraper.headers["User-Agent"] = random.choice(UA_ROTATE)
            scraper.headers["Referer"] = referer
            # bust cache / vary fingerprint
            rnd = str(random.randint(10**6, 10**7 - 1))
            sep = "&" if "?" in page_url else "?"
            url_with_rnd = f"{page_url}{sep}_rnd={rnd}"

            r = scraper.get(url_with_rnd, timeout=REQUEST_TIMEOUT, allow_redirects=True)
            status = r.status_code
            if status >= 500:
                raise RuntimeError(f"{status} Server Error")
            if status in (403, 429):
                raise RuntimeError(f"{status} Rate/Captcha")
            return r.text
        except Exception as e:
            last_err = e
            log.warning(f"–û—à–∏–±–∫–∞ HTML –Ω–∞ {page_url} (attempt {attempt}/{RETRIES_PER_PAGE}): {e}")
            time.sleep(random.uniform(*RETRY_SLEEP))
    raise last_err or RuntimeError("–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")

def parse_items_from_html(html: str):
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–µ–∫:
      - –æ—Å–Ω–æ–≤–Ω–æ–π: li.s-item (eBay classic)
      - –∑–∞–ø–∞—Å–Ω–æ–π: div.s-item__wrapper –∏–ª–∏ [data-testid='item-card'] (–Ω–æ–≤–∞—è –≤–µ—Ä—Å—Ç–∫–∞)
    """
    soup = BeautifulSoup(html, "html.parser")
    full_text_low = soup.get_text(" ", strip=True).lower()

    if looks_like_human_check(soup, full_text_low):
        return [], True

    # –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–∏—Å–æ–∫
    nodes = soup.select("li.s-item")
    # –§–æ–ª–ª–±—ç–∫–∏
    if len(nodes) < 5:
        alt = soup.select("div.s-item__wrapper, [data-testid='item-card']")
        if len(alt) > len(nodes):
            nodes = alt

    items = []
    for n in nodes:
        a = n.select_one("a.s-item__link") or n.select_one("a[href*='/itm/']")
        title_tag = n.select_one("h3.s-item__title") or n.select_one("[data-testid='item-title']")
        price_tag = n.select_one(".s-item__price") or n.select_one("[data-testid='item-price']")
        if not a or not title_tag:
            continue

        link = a.get("href", "").strip()
        title = title_tag.get_text(strip=True)
        price = price_tag.get_text(strip=True) if price_tag else ""

        # itemId –∏–∑ —Å—Å—ã–ª–∫–∏ /itm/1234567890?
        m = re.search(r"/itm/(\d{9,})\b", link) or re.search(r"/(\d{9,})\?", link)
        item_id = m.group(1) if m else link

        items.append({"id": item_id, "title": title, "price": price, "link": link})

    return items, False

# –£–∂–µ —É–≤–∏–¥–µ–Ω–Ω—ã–µ ID –ø–æ —Å—Å—ã–ª–∫–µ –ø–æ–∏—Å–∫–∞
seen = {}

def crawl_search(url: str):
    base = normalize_search_url(url)
    log.info(f"Base URL: {base}")
    scraper = make_scraper()

    all_items = []
    saw_human_check = False

    for p in range(1, PAGES_TO_SCAN + 1):
        page_url = f"{base}&_pgn={p}"
        log.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {p}/{PAGES_TO_SCAN}: {page_url}")

        try:
            html = fetch_page_html(scraper, page_url, referer=base)
        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ HTML p={p}: {e}")
            continue

        items, is_human = parse_items_from_html(html)
        log.info(f"HTML p={p}: –Ω–∞–π–¥–µ–Ω–æ {len(items)} –∫–∞—Ä—Ç–æ—á–µ–∫")
        all_items.extend(items)
        saw_human_check = saw_human_check or is_human

        if len(items) < 5:  # –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –º–∞–ª–æ ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏–º –¥–∞–º–ø
            try:
                dump_path = f"/opt/render/project/src/ebay_debug_p{p}.html"
                with open(dump_path, "w", encoding="utf-8") as f:
                    f.write(html)
                log.warning(f"–°–æ—Ö—Ä–∞–Ω—ë–Ω HTML –¥–∞–º–ø –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏: {dump_path}")
            except Exception as e:
                log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–º–ø HTML: {e}")

        time.sleep(random.uniform(1.2, 2.8))

    # –£–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º
    uniq = {}
    for it in all_items:
        uniq[it["id"]] = it
    all_items = list(uniq.values())

    if saw_human_check:
        log.warning("–ü–æ—Ö–æ–∂–µ, –æ—Ç–¥–∞–Ω–∞ –∑–∞—â–∏—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (captcha/human check).")

    log.info(f"–ò—Ç–æ–≥: —Å–æ–±—Ä–∞–Ω–æ {len(all_items)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–ø–æ—Å–ª–µ —á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)")
    return all_items

def main():
    global seen

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    for url in EBAY_URLS:
        try:
            items = crawl_search(url)
            seen[url] = {it["id"] for it in items}
            log.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(seen[url])} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ {url}")
        except Exception as e:
            log.warning(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {url}: {e}")
            seen[url] = set()

    check_num = 0
    while True:
        check_num += 1
        log.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ #{check_num} –Ω–∞—á–∞—Ç–∞")
        for url in EBAY_URLS:
            try:
                items = crawl_search(url)
                new_items = [it for it in items if it["id"] not in seen[url]]
                log.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(items)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –Ω–æ–≤—ã—Ö={len(new_items)} –ø–æ {url}")

                # –û—Ç–º–µ—á–∞–µ–º —É–≤–∏–¥–µ–Ω–Ω—ã–µ
                for it in new_items:
                    seen[url].add(it["id"])

                # –ó–¥–µ—Å—å –ø–æ–∑–∂–µ –≤–µ—Ä–Ω—ë–º Telegram
                # for it in new_items:
                #     send_telegram_message(f"üÜï {it['title']}\n{it['price']}\n{it['link']}")

            except Exception as e:
                log.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è {url}: {e}")

            time.sleep(random.uniform(1.0, 2.0))

        time.sleep(CHECK_INTERVAL)

if __name__ == "__main__":
    log.info(f"–°–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω. –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–æ–∫: {CHECK_INTERVAL}s")
    main()
