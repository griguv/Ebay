import os
import re
import json
import time
import logging
import requests
import cloudscraper
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse, urlsplit, parse_qsl

# ================== –ù–ê–°–¢–†–û–ô–ö–ò ==================
EBAY_URLS = [
    # –º–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–ª—è—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ —Å—Å—ã–ª–∫–∏ ‚Äî –∫–æ–¥ —Å–∞–º —É–±–µ—Ä—ë—Ç _stpos –∏ _fcid
    "https://www.ebay.com/sch/i.html?_udlo=100&_nkw=garmin+astro+320+&_sacat=0&_fcid=1&_stpos=19720",
]

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_IDS = [c.strip() for c in os.getenv("CHAT_ID", "").split(",") if c.strip()]

CHECK_INTERVAL = 180           # –∫–∞–∂–¥—ã–µ 3 –º–∏–Ω
REPORT_INTERVAL = 1800         # –æ—Ç—á—ë—Ç —Ä–∞–∑ –≤ 30 –º–∏–Ω
ERROR_NOTIFY_INTERVAL = 1800   # —É–≤–µ–¥–æ–º–ª—è—Ç—å –æ–± –æ—à–∏–±–∫–∞—Ö –Ω–µ —á–∞—â–µ —á–µ–º —Ä–∞–∑ –≤ 30 –º–∏–Ω
REQUEST_TIMEOUT = 25
MAX_PAGES = 3                  # –ø–∞–≥–∏–Ω–∞—Ü–∏—è: –¥–æ 3 —Å—Ç—Ä–∞–Ω–∏—Ü
RSS_FALLBACK_THRESHOLD = 5     # –µ—Å–ª–∏ HTML –¥–∞–ª < 5 –æ–±—ä—è–≤–ª–µ–Ω–∏–π ‚Äî –ø—Ä–æ–±—É–µ–º RSS

HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36"),
    "Accept-Language": "en-US,en;q=0.9",
}

# ================== –õ–û–ì–ò ==================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    force=True,
)
log = logging.getLogger("ebay-bot")

# ================== –ì–õ–û–ë–ê–õ–¨–ù–´–ï ==================
scraper = cloudscraper.create_scraper()
seen_items = {url: set() for url in EBAY_URLS}
last_error_time = datetime.min
last_report_time = datetime.now()
checks_count = 0
new_items_count = 0

# ================== –£–¢–ò–õ–ò–¢–´ ==================
def _strip_params(url: str, keys_to_drop) -> str:
    """–£–±–∏—Ä–∞–µ–º –∏–∑ URL —É–∫–∞–∑–∞–Ω–Ω—ã–µ GET-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä _stpos, _fcid)."""
    u = urlparse(url)
    q = dict(parse_qsl(u.query, keep_blank_values=True))
    for k in keys_to_drop:
        q.pop(k, None)
    new_q = urlencode(q, doseq=True)
    return urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))

def _clean_base_url(url: str) -> str:
    """–ì–æ—Ç–æ–≤–∏–º –±–∞–∑–æ–≤—ã–π URL –ø–æ–∏—Å–∫–∞: –±–µ–∑ –≥–µ–æ-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —Å _ipg=240 –∏ rt=nc."""
    url = _strip_params(url, ["_stpos", "_fcid"])
    u = urlparse(url)
    q = dict(parse_qsl(u.query, keep_blank_values=True))
    q.pop("_pgn", None)
    q["_ipg"] = "240"
    q["rt"] = "nc"
    new_q = urlencode(q, doseq=True)
    base = urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))
    return base

def _extract_item_id(link: str) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π id:
    1) –∏–∑ query (item id –≤—Ä–æ–¥–µ 'mkevt' / 'epid' / 'itm' / 'hash' —Å item=XXXX);
    2) –∏–∑ –ø—É—Ç–∏ /itm/<id>;
    3) –∏–∑ –∫–æ–Ω—Ü–∞ —Å—Å—ã–ª–∫–∏;
    4) fallback: —Å–∞–º–∞ —Å—Å—ã–ª–∫–∞ —Ü–µ–ª–∏–∫–æ–º.
    """
    try:
        # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã eBay
        # 1) .../itm/1234567890
        m = re.search(r"/itm/(\d{8,})", link)
        if m:
            return m.group(1)

        # 2) –ø–∞—Ä–∞–º–µ—Ç—Ä 'item' –≤ query
        qs = parse_qs(urlsplit(link).query)
        for key in ("item", "itemid", "itm", "nid"):
            if key in qs and qs[key]:
                cand = qs[key][0]
                if cand.isdigit():
                    return cand

        # 3) –•–≤–æ—Å—Ç URL –¥–æ '?'
        tail = link.split("?")[0].rstrip("/").split("/")[-1]
        if tail and any(ch.isalnum() for ch in tail):
            return tail

    except Exception:
        pass
    return link  # —Å–∞–º—ã–π —Å—Ç–∞–±–∏–ª—å–Ω—ã–π fallback ‚Äî –≤—Å—è —Å—Å—ã–ª–∫–∞

def _post_telegram(text: str):
    if not BOT_TOKEN or not CHAT_IDS:
        return
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    for cid in CHAT_IDS:
        try:
            r = requests.post(url, data={"chat_id": cid, "text": text}, timeout=15)
            if r.status_code != 200:
                log.warning(f"–û—à–∏–±–∫–∞ Telegram ({cid}): {r.text}")
        except Exception as e:
            log.warning(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ Telegram ({cid}): {e}")

# ================== –ü–ê–†–°–ò–ù–ì ==================
def _parse_json_ld(soup: BeautifulSoup):
    items = []
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            blob = script.string
            if not blob:
                continue
            data = json.loads(blob)
            # –í–∞—Ä–∏–∞–Ω—Ç —Å–ø–∏—Å–∫–∞
            if isinstance(data, list):
                for node in data:
                    items.extend(_items_from_json_node(node))
            # –í–∞—Ä–∏–∞–Ω—Ç —Å–ª–æ–≤–∞—Ä—è
            elif isinstance(data, dict):
                items.extend(_items_from_json_node(data))
        except Exception:
            continue
    return items

def _items_from_json_node(node):
    items = []
    # —Ñ–æ—Ä–º–∞—Ç—ã –±—ã–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ: ItemList -> itemListElement[], –ª–∏–±–æ —Å—Ä–∞–∑—É Offer/Products
    try:
        if isinstance(node, dict):
            if "itemListElement" in node:
                for el in node["itemListElement"]:
                    product = el.get("item") or el.get("url") or el
                    rec = _item_from_json_product(product)
                    if rec:
                        items.append(rec)
            else:
                rec = _item_from_json_product(node)
                if rec:
                    items.append(rec)
    except Exception:
        pass
    return items

def _item_from_json_product(prod):
    try:
        if isinstance(prod, str):
            link = prod
            title = ""
            price = ""
        elif isinstance(prod, dict):
            link = prod.get("url") or prod.get("link") or ""
            title = prod.get("name") or prod.get("title") or ""
            offers = prod.get("offers") or {}
            if isinstance(offers, dict):
                price = ((offers.get("priceCurrency") or "") + " " + (offers.get("price") or "")).strip()
            else:
                price = ""
        else:
            return None

        if not link:
            return None
        item_id = _extract_item_id(link)
        if not title:
            # –∏–Ω–æ–≥–¥–∞ –µ—Å—Ç—å "description" –≤–º–µ—Å—Ç–æ –∏–º–µ–Ω–∏
            title = (prod.get("description") or "").strip() if isinstance(prod, dict) else ""
        title = title[:300]
        return {"id": item_id, "title": title or "(no title)", "price": price or "", "link": link}
    except Exception:
        return None

def _parse_html_items(soup: BeautifulSoup):
    """
    HTML-–ø–∞—Ä—Å–∏–Ω–≥: –ø—Ä–æ–±—É–µ–º —Å—Ä–∞–∑—É –¥–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞,
    —á—Ç–æ–±—ã –æ—Ö–≤–∞—Ç–∏—Ç—å —Ä–∞–∑–Ω—ã–µ —Ä–∞—Å–∫–ª–∞–¥–∫–∏ eBay.
    """
    items = []

    # –í–∞—Ä–∏–∞–Ω—Ç 1: –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫
    nodes = soup.select("li.s-item")
    # –í–∞—Ä–∏–∞–Ω—Ç 2: –∏–Ω–æ–≥–¥–∞ eBay –≤–∫–ª–∞–¥—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä—å .srp-results
    nodes2 = soup.select(".srp-results .s-item")
    if len(nodes2) > len(nodes):
        nodes = nodes2

    for card in nodes:
        try:
            # –ò–Ω–æ–≥–¥–∞ eBay –¥–æ–±–∞–≤–ª—è–µ—Ç "Sponsored", –∏—Ö –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞–µ–º
            badge = card.select_one(".s-item__title--tagblock, .s-item__title--tag")
            if badge and "sponsored" in badge.get_text(" ").lower():
                continue

            title_tag = card.select_one(".s-item__title")
            link_tag = card.select_one("a.s-item__link, a.s-item__title")
            price_tag = card.select_one(".s-item__price")

            if not link_tag:
                continue

            link = link_tag.get("href", "").strip()
            if not link:
                continue

            title = title_tag.get_text(strip=True) if title_tag else ""
            price = price_tag.get_text(strip=True) if price_tag else ""
            item_id = _extract_item_id(link)

            items.append({"id": item_id, "title": title or "(no title)", "price": price, "link": link})
        except Exception:
            continue

    return items

def _parse_rss(url: str):
    items = []
    rss_url = url + ("&" if ("?" in url) else "?") + "_rss=1"
    try:
        resp = scraper.get(rss_url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "xml")

        for it in soup.find_all("item"):
            title = it.title.get_text(strip=True) if it.title else ""
            link = it.link.get_text(strip=True) if it.link else ""
            if not link:
                continue
            # –¶–µ–Ω–∞ –≤ description:
            desc = it.description.get_text(strip=True) if it.description else ""
            m = re.search(r"\$\s?[\d,]+(?:\.\d+)?", desc)
            price = m.group(0) if m else ""
            item_id = _extract_item_id(link)
            items.append({"id": item_id, "title": title or "(no title)", "price": price, "link": link})
    except Exception as e:
        log.warning(f"–û—à–∏–±–∫–∞ RSS: {e}")

    return items

def fetch_listings(search_url: str):
    """
    –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:
      1) JSON-LD
      2) HTML (.s-item)
      3) RSS (–µ—Å–ª–∏ HTML –¥–∞–ª –º–∞–ª–æ)
    """
    base = _clean_base_url(search_url)
    log.info(f"Base URL: {base}")

    aggregated = []
    samples_for_log = []  # —Å–æ–±–µ—Ä—ë–º –ø—Ä–∏–º–µ—Ä—ã

    for p in range(1, MAX_PAGES + 1):
        page_url = f"{base}&_pgn={p}"
        try:
            log.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {p}/{MAX_PAGES}: {page_url}")
            resp = scraper.get(page_url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            items = _parse_json_ld(soup)
            if not items:
                items = _parse_html_items(soup)

            log.info(f"HTML p={p}: –Ω–∞–π–¥–µ–Ω–æ {len(items)}")
            aggregated.extend(items)

            # —Å–æ–±–µ—Ä—ë–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ª–æ–≥–æ–≤ (—Ç–æ–ª—å–∫–æ 3 —à—Ç. –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
            for it in items[:3]:
                samples_for_log.append(f"‚Ä¢ {it['title'][:60]} ‚Äî {it['price']}")

            # –∏–Ω–æ–≥–¥–∞ –ø—É—Å—Ç–∞—è —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ‚Äî –≤—ã—Ö–æ–¥–∏–º
            if len(items) == 0 and p > 1:
                break
        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ HTML p={p}: {e}")

    if aggregated and samples_for_log:
        log.info("–ü—Ä–∏–º–µ—Ä—ã (–ø–µ—Ä–≤—ã–µ 3 –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ):\n" + "\n".join(samples_for_log[:9]))

    # –µ—Å–ª–∏ HTML –¥–∞–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ ‚Äî –ø—Ä–æ–±—É–µ–º RSS
    if len(aggregated) < RSS_FALLBACK_THRESHOLD:
        rss_items = _parse_rss(base)
        log.warning(f"HTML –¥–∞–ª –º–∞–ª–æ ({len(aggregated)}) ‚Äî RSS –≤–µ—Ä–Ω—É–ª {len(rss_items)}")
        aggregated.extend(rss_items)

    # –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ ID (–µ—Å–ª–∏ ID –ø—É—Å—Ç–æ–π ‚Äî –ø–æ —Å—Å—ã–ª–∫–µ –æ–Ω —É–∂–µ —Ä–∞–≤–µ–Ω –≤—Å–µ–π —Å—Å—ã–ª–∫–µ)
    uniq = {}
    for it in aggregated:
        uniq[it["id"]] = it

    log.info(f"–ò—Ç–æ–≥: —Å–æ–±—Ä–∞–Ω–æ {len(uniq)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (–ø–æ—Å–ª–µ JSON/HTML + RSS)")
    return list(uniq.values())

# ================== –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ==================
def main():
    global last_report_time, checks_count, new_items_count, last_error_time

    log.info(f"–°–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω. –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–æ–∫: {CHECK_INTERVAL}s")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî –Ω–µ —É–≤–µ–¥–æ–º–ª—è–µ–º, –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–º–∏–Ω–∞–µ–º
    for url in EBAY_URLS:
        try:
            items = fetch_listings(url)
            for it in items:
                seen_items[url].add(it["id"])
            log.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(items)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ {url}")
        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ {url}: {e}")

    while True:
        checks_count += 1
        log.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ #{checks_count} –Ω–∞—á–∞—Ç–∞")

        for url in EBAY_URLS:
            try:
                items = fetch_listings(url)
                new_count = 0
                for it in items:
                    if it["id"] not in seen_items[url]:
                        seen_items[url].add(it["id"])
                        new_items_count += 1
                        new_count += 1
                        msg = (
                            "üÜï –ù–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –Ω–∞ eBay!\n"
                            f"üìå {it['title']}\n"
                            f"üí≤ {it['price']}\n"
                            f"üîó {it['link']}"
                        )
                        _post_telegram(msg)
                log.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(items)} –≤–∞–ª–∏–¥–Ω—ã—Ö, –Ω–æ–≤—ã—Ö={new_count} –ø–æ {url}")
            except Exception as e:
                log.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {url}: {e}")
                if datetime.now() - last_error_time > timedelta(seconds=ERROR_NOTIFY_INTERVAL):
                    _post_telegram(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {url}: {e}")
                    last_error_time = datetime.now()

        # –æ—Ç—á—ë—Ç —Ä–∞–∑ –≤ 30 –º–∏–Ω—É—Ç
        if datetime.now() - last_report_time > timedelta(seconds=REPORT_INTERVAL):
            report = (
                "üìä –û—Ç—á—ë—Ç –∑–∞ 30 –º–∏–Ω—É—Ç\n"
                f"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"üîé –ü—Ä–æ–≤–µ—Ä–æ–∫: {checks_count}\n"
                f"üÜï –ù–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {new_items_count}\n"
                "‚úÖ –ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç"
            )
            _post_telegram(report)
            last_report_time = datetime.now()
            checks_count = 0
            new_items_count = 0

        time.sleep(CHECK_INTERVAL)

if __name__ == "__main__":
    main()
