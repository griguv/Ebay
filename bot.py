import os
import time
import json
import logging
import requests
import cloudscraper
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

# ================= –ù–ê–°–¢–†–û–ô–ö–ò =================
EBAY_URLS = [
    "https://www.ebay.com/sch/i.html?_udlo=100&_nkw=garmin+astro+320+&_sacat=0&_fcid=1",
]

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_IDS = os.getenv("CHAT_ID", "").split(",")

CHECK_INTERVAL = 180          # –∫–∞–∂–¥—ã–µ 3 –º–∏–Ω
REPORT_INTERVAL = 1800        # –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω
ERROR_NOTIFY_INTERVAL = 1800  # —Ä–∞–∑ –≤ 30 –º–∏–Ω –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
MAX_PAGES = 3                 # –¥–æ 3 —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–∞ –∫–∞–∂–¥—ã–π –ø–æ–∏—Å–∫
REQUEST_TIMEOUT = 20

# ================= –õ–û–ì–ò =================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    force=True
)

# ================= –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï =================
scraper = cloudscraper.create_scraper()
seen_items = {url: set() for url in EBAY_URLS}
last_error_time = datetime.min
last_report_time = datetime.now()
checks_count = 0
new_items_count = 0

HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36")
}

# ================= –£–¢–ò–õ–ò–¢–´ =================
def clean_base_url(url):
    """–£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –¥–æ–±–∞–≤–ª—è–µ–º _ipg=240&rt=nc"""
    parsed = urlparse(url)
    qs = parse_qs(parsed.query)
    qs.pop("_pgn", None)
    qs["_ipg"] = ["240"]
    qs["rt"] = ["nc"]
    new_query = urlencode(qs, doseq=True)
    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, "", new_query, ""))

def send_telegram_message(message):
    if not BOT_TOKEN:
        logging.warning("BOT_TOKEN –Ω–µ –∑–∞–¥–∞–Ω")
        return
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    for raw_id in CHAT_IDS:
        chat_id = raw_id.strip()
        if not chat_id:
            continue
        try:
            r = requests.post(url, data={"chat_id": chat_id, "text": message}, timeout=10)
            if r.status_code != 200:
                logging.warning(f"–û—à–∏–±–∫–∞ Telegram ({chat_id}): {r.text}")
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ Telegram ({chat_id}): {e}")

# ================= –ü–ê–†–°–ò–ù–ì =================
def parse_json_ld(soup):
    """–ü–∞—Ä—Å–∏–Ω–≥ JSON –≤–Ω—É—Ç—Ä–∏ <script type=application/ld+json>"""
    items = []
    for script in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(script.string.strip())
            if isinstance(data, dict) and "itemListElement" in data:
                for el in data["itemListElement"]:
                    node = el.get("item", {})
                    if not node:
                        continue
                    title = node.get("name")
                    link = node.get("url")
                    offers = node.get("offers", {})
                    price = offers.get("priceCurrency", "") + " " + offers.get("price", "")
                    if title and link:
                        item_id = link.split("/")[-1].split("?")[0]
                        items.append({"id": item_id, "title": title, "price": price, "link": link})
        except Exception:
            continue
    return items

def parse_html_items(soup):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–∞—Ä—ã–º —Å–ø–æ—Å–æ–±–æ–º: .s-item"""
    items = []
    for card in soup.select(".s-item"):
        title_tag = card.select_one(".s-item__title")
        price_tag = card.select_one(".s-item__price")
        link_tag = card.select_one(".s-item__link")
        if not title_tag or not price_tag or not link_tag:
            continue
        title = title_tag.get_text(strip=True)
        price = price_tag.get_text(strip=True)
        link = link_tag.get("href", "")
        item_id = link.split("/")[-1].split("?")[0] or link
        items.append({"id": item_id, "title": title, "price": price, "link": link})
    return items

def parse_rss(url):
    """–ü–∞—Ä—Å–∏–Ω–≥ RSS –ª–µ–Ω—Ç—ã eBay"""
    rss_url = url + "&_rss=1"
    items = []
    try:
        resp = scraper.get(rss_url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "xml")
        for item in soup.find_all("item"):
            title = item.title.get_text(strip=True) if item.title else None
            link = item.link.get_text(strip=True) if item.link else None
            price = ""
            desc = item.description.get_text(strip=True) if item.description else ""
            if "$" in desc:
                price = desc.split("$")[-1].split("<")[0]
                price = "$" + price
            if title and link:
                item_id = link.split("/")[-1].split("?")[0]
                items.append({"id": item_id, "title": title, "price": price, "link": link})
    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ RSS: {e}")
    return items

def fetch_listings(base_url):
    """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: JSON-LD ‚Üí HTML ‚Üí RSS"""
    all_items = []
    base_url = clean_base_url(base_url)
    logging.info(f"Base URL: {base_url}")

    for page in range(1, MAX_PAGES + 1):
        page_url = f"{base_url}&_pgn={page}"
        try:
            logging.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}/{MAX_PAGES}: {page_url}")
            resp = scraper.get(page_url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            items = parse_json_ld(soup)
            if not items:
                items = parse_html_items(soup)

            logging.info(f"HTML p={page}: {len(items)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            all_items.extend(items)
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ HTML p={page}: {e}")

    # fallback: –µ—Å–ª–∏ –º–∞–ª–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π, –ø—Ä–æ–±—É–µ–º RSS
    if len(all_items) < 5:
        rss_items = parse_rss(base_url)
        logging.warning(f"HTML –¥–∞–ª –º–∞–ª–æ ({len(all_items)}) ‚Äî –ø—Ä–æ–±—É—é RSS: {len(rss_items)}")
        all_items.extend(rss_items)

    uniq = {it["id"]: it for it in all_items}
    logging.info(f"–ò—Ç–æ–≥: —Å–æ–±—Ä–∞–Ω–æ {len(uniq)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π (HTML+RSS)")
    return list(uniq.values())

# ================= –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ =================
if __name__ == "__main__":
    logging.info(f"–°–µ—Ä–≤–∏—Å –∑–∞–ø—É—â–µ–Ω. –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–æ–∫: {CHECK_INTERVAL}s")

    # –ø–µ—Ä–≤–∏—á–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    for url in EBAY_URLS:
        try:
            listings = fetch_listings(url)
            for item in listings:
                seen_items[url].add(item["id"])
            logging.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(listings)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ {url}")
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ {url}: {e}")

    while True:
        checks_count += 1
        logging.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ #{checks_count} –Ω–∞—á–∞—Ç–∞")

        for url in EBAY_URLS:
            try:
                listings = fetch_listings(url)
                new_count = 0
                for item in listings:
                    if item["id"] not in seen_items[url]:
                        seen_items[url].add(item["id"])
                        new_items_count += 1
                        new_count += 1
                        msg = (
                            "üÜï –ù–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ!\n"
                            f"üìå {item['title']}\n"
                            f"üí≤ {item['price']}\n"
                            f"üîó {item['link']}"
                        )
                        send_telegram_message(msg)
                logging.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(listings)} –≤–∞–ª–∏–¥–Ω—ã—Ö, –Ω–æ–≤—ã—Ö={new_count} –ø–æ {url}")
            except Exception as e:
                logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {url}: {e}")
                if datetime.now() - last_error_time > timedelta(seconds=ERROR_NOTIFY_INTERVAL):
                    send_telegram_message(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {url}: {e}")
                    last_error_time = datetime.now()

        # –æ—Ç—á—ë—Ç –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω
        if datetime.now() - last_report_time > timedelta(seconds=REPORT_INTERVAL):
            report = (
                "üìä –û—Ç—á—ë—Ç –∑–∞ 30 –º–∏–Ω—É—Ç\n"
                f"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"üîé –ü—Ä–æ–≤–µ—Ä–æ–∫: {checks_count}\n"
                f"üÜï –ù–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {new_items_count}\n"
                "‚úÖ –ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç"
            )
            send_telegram_message(report)
            last_report_time = datetime.now()
            checks_count = 0
            new_items_count = 0

        time.sleep(CHECK_INTERVAL)
